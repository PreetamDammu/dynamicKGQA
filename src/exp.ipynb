{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from utils.prompts import generate_question_evaluation_prompt_v2\n",
    "from utils.bedrock_functions import invoke_bedrock_endpoint, build_anthropic_request_body\n",
    "from utils.bedrock_functions import build_mistral_request_body, build_nova_request_body, build_llama_request_body\n",
    "\n",
    "from utils.misc_helpers import read_jsonl_file, save_as_jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/ec2-user/preetam_experiments\"\n",
    "split = 'test'\n",
    "data = json.load(open(f\"{directory}/batch_processing_files/{split}/intermediate_res/qa_pairs_{split}.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_jsonl_file('/home/ec2-user/code_repos/dynamicKGQA/inputs/annotator_requests/tests/annotator_requests_qcheck_mistral_test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recordId': '00009785500',\n",
       " 'modelInput': {'modelId': 'mistral.mistral-small-2402-v1:0',\n",
       "  'contentType': 'application/json',\n",
       "  'accept': 'application/json',\n",
       "  'body': '{\"prompt\": \"<s>[INST] As an expert evaluator, your role is to assess the quality and validity of trivia or natural questions. These questions aim to test the responder\\'s knowledge, which may require implicit or external information. Your goal is to analyze the question based on the following criteria:\\\\n\\\\n- **Logical Structure**: Verify if the grammar and syntax are correct. (True if grammatically and syntactically correct; False if there are issues with grammar or syntax.)\\\\n- **Redundancy**: Confirm that the question does not contain its own answer explicitly or through overly obvious phrasing. (True if it contains its answer; False if it does not.)\\\\n- **Multiple Answers**: Determine if the question allows for multiple valid answers. This is acceptable in some cases, but flag it if it reduces the question\\'s effectiveness or specificity. (True if multiple answers are plausible; False if only one valid answer is expected.)\\\\n    \\\\n#### Output JSON Keys:\\\\n- `question`: The input question.\\\\n- `logical_structure_flag`: (True/False)\\\\n- `logical_structure_reasoning`: Reason for the logical structure flag.\\\\n- `redundancy_flag`: (True/False)\\\\n- `redundancy_reasoning`: Reason for the redundancy flag.\\\\n- `multiple_answers_flag`: (True/False)\\\\n- `multiple_answers_reasoning`: Reason for the multiple answers flag.\\\\n\\\\n#### Task:\\\\nAnalyze the following question and provide a JSON object containing flags and reasons for potential issues:\\\\n\\\\n**Question**: \\\\\"What family does the alpaca belong to, considering its evolutionary lineage through its parent taxa?\\\\\"\\\\n\\\\n#### Output:\\\\nReturn a JSON object that evaluates the question based on the criteria above.\\\\n [/INST]\", \"max_tokens\": 2048, \"temperature\": 0}'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': [{'text': ' {\\n  \"question\": \"What family does the alpaca belong to, considering its evolutionary lineage through its parent taxa?\",\\n  \"logical_structure_flag\": true,\\n  \"logical_structure_reasoning\": \"The question is grammatically and syntactically correct.\",\\n  \"redundancy_flag\": false,\\n  \"redundancy_reasoning\": \"The question does not contain its own answer explicitly or through overly obvious phrasing.\",\\n  \"multiple_answers_flag\": false,\\n  \"multiple_answers_reasoning\": \"The question is expected to have one valid answer, which is the taxonomic family to which the alpaca belongs.\"\\n}', 'stop_reason': 'stop'}]}\n"
     ]
    }
   ],
   "source": [
    "request_body = test[0]['modelInput']\n",
    "response = invoke_bedrock_endpoint(\n",
    "    request_body=request_body[\"body\"],\n",
    "    model_id=request_body[\"modelId\"],\n",
    "    contentType=request_body[\"contentType\"]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_var = []\n",
    "for i in range(107):\n",
    "    temp = test[i]\n",
    "    temp['modelInput'] = {'recordId': temp['recordId'],\n",
    "                          'modelInput': temp['modelInput']['body']}\n",
    "    test_var.append(test[i]['modelInput'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<s>[INST] As an expert evaluator, your role is to assess the quality and validity of trivia or natural questions. These questions aim to test the responder\\'s knowledge, which may require implicit or external information. Your goal is to analyze the question based on the following criteria:\\n\\n- **Logical Structure**: Verify if the grammar and syntax are correct. (True if grammatically and syntactically correct; False if there are issues with grammar or syntax.)\\n- **Redundancy**: Confirm that the question does not contain its own answer explicitly or through overly obvious phrasing. (True if it contains its answer; False if it does not.)\\n- **Multiple Answers**: Determine if the question allows for multiple valid answers. This is acceptable in some cases, but flag it if it reduces the question\\'s effectiveness or specificity. (True if multiple answers are plausible; False if only one valid answer is expected.)\\n    \\n#### Output JSON Keys:\\n- `question`: The input question.\\n- `logical_structure_flag`: (True/False)\\n- `logical_structure_reasoning`: Reason for the logical structure flag.\\n- `redundancy_flag`: (True/False)\\n- `redundancy_reasoning`: Reason for the redundancy flag.\\n- `multiple_answers_flag`: (True/False)\\n- `multiple_answers_reasoning`: Reason for the multiple answers flag.\\n\\n#### Task:\\nAnalyze the following question and provide a JSON object containing flags and reasons for potential issues:\\n\\n**Question**: \"What family does the alpaca belong to, considering its evolutionary lineage through its parent taxa?\"\\n\\n#### Output:\\nReturn a JSON object that evaluates the question based on the criteria above.\\n [/INST]',\n",
       " 'max_tokens': 2048,\n",
       " 'temperature': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(test_var[0]['modelInput'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_jsonl(test_var, '/home/ec2-user/code_repos/dynamicKGQA/outputs/annotations/test/annotator_requests_qcheck_mistral_test100.jsonl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claimbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
